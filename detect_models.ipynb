{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import the necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2292d0289c70b472"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:26:44.348177Z",
     "start_time": "2024-03-07T09:26:37.585372Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import and preprocess the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4147e65fb555b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = 'Data/dataset_5.csv'\n",
    "detectionData = pd.read_csv(data)\n",
    "\n",
    "X_columns = [\"Ia\",\"Ib\", \"Ic\" ,\"Va\", \"Vb\" ,\"Vc\"]\n",
    "X_detection = detectionData[X_columns]\n",
    "y_columns = [\"F\"]\n",
    "y_detection = detectionData[y_columns]\n",
    "additional_columns = [\"A\", \"B\", \"C\", \"G\", \"L\", \"Rf\"]\n",
    "additional_data = detectionData[additional_columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test, additional_data_train, additional_data_test = train_test_split(X_detection, y_detection, additional_data, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:26:44.525890Z",
     "start_time": "2024-03-07T09:26:44.350309Z"
    }
   },
   "id": "bff2c987ecb093ad",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define training and evaluation functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfa8307cb747e359"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define a dictionary to store the cross-validation metrics of the algorithms\n",
    "cv_metrics = {'Model': [], 'Accuracy': []}\n",
    "test_metrics = {'Model': [], 'Accuracy': []}\n",
    "\n",
    "# Define a function to train and evaluate each algorithm\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train):\n",
    "    # Define the scoring metrics for multi-class classification\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "    }\n",
    "\n",
    "    # Perform cross-validation using StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring)\n",
    "    \n",
    "    # Store the cross-validation metrics\n",
    "    cv_metrics['Model'].append(model_name)\n",
    "    cv_metrics['Accuracy'].append(scores['test_accuracy'].mean())\n",
    "    print(f\"{model_name}: Cross-validation metrics calculated\")\n",
    "    \n",
    "    # Fit the model on the entire training set\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Define a function to evaluate the model on the test set and store the metrics\n",
    "def evaluate_on_test_set(model, model_name, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_metrics['Model'].append(model_name)\n",
    "    test_metrics['Accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "    print(f\"{model_name}: Test metrics calculated\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:26:44.531741Z",
     "start_time": "2024-03-07T09:26:44.526960Z"
    }
   },
   "id": "55b2f751a425c684",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f0fdd4ca6703d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "models = [\n",
    "    (LogisticRegression(random_state=42, max_iter=1000), \"Logistic Regression\"),\n",
    "    (SVC(random_state=42), \"Support Vector Machines\"),\n",
    "    (KNeighborsClassifier(), \"K-Nearest Neighbors\"),\n",
    "    (DecisionTreeClassifier(random_state=42), \"Decision Trees\"),\n",
    "    (RandomForestClassifier(random_state=42), \"Random Forest\"),\n",
    "    (GradientBoostingClassifier(random_state=42), \"Gradient Boosting\"),\n",
    "    (MLPClassifier(random_state=42, max_iter=1000), \"Neural Networks\"),\n",
    "    (GaussianNB(), \"Naive Bayes\"),\n",
    "    (AdaBoostClassifier(random_state=42), \"AdaBoost\"),\n",
    "    (XGBClassifier(random_state=42), \"XGBoost\"),\n",
    "    (LGBMClassifier(random_state=42), \"LightGBM\"),\n",
    "    (CatBoostClassifier(random_state=42, verbose=0), \"CatBoost\"),\n",
    "    (detectionANN, \"Artificial Neural Network\")\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:26:44.595217Z",
     "start_time": "2024-03-07T09:26:44.533096Z"
    }
   },
   "id": "df6db12ee86289f7",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train and evaluate the models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9fb4b1b1fc5b051"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Cross-validation metrics calculated\n",
      "Logistic Regression: Test metrics calculated\n",
      "Support Vector Machines: Cross-validation metrics calculated\n",
      "Support Vector Machines: Test metrics calculated\n",
      "K-Nearest Neighbors: Cross-validation metrics calculated\n",
      "K-Nearest Neighbors: Test metrics calculated\n",
      "Decision Trees: Cross-validation metrics calculated\n",
      "Decision Trees: Test metrics calculated\n",
      "Random Forest: Cross-validation metrics calculated\n",
      "Random Forest: Test metrics calculated\n",
      "Gradient Boosting: Cross-validation metrics calculated\n",
      "Gradient Boosting: Test metrics calculated\n",
      "Neural Networks: Cross-validation metrics calculated\n",
      "Neural Networks: Test metrics calculated\n",
      "Naive Bayes: Cross-validation metrics calculated\n",
      "Naive Bayes: Test metrics calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonre\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jonre\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jonre\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jonre\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jonre\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: Cross-validation metrics calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonre\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: Test metrics calculated\n",
      "XGBoost: Cross-validation metrics calculated\n",
      "XGBoost: Test metrics calculated\n",
      "[LightGBM] [Info] Number of positive: 26698, number of negative: 26806\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 53504, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498991 -> initscore=-0.004037\n",
      "[LightGBM] [Info] Start training from score -0.004037\n",
      "[LightGBM] [Info] Number of positive: 26698, number of negative: 26806\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 53504, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498991 -> initscore=-0.004037\n",
      "[LightGBM] [Info] Start training from score -0.004037\n",
      "[LightGBM] [Info] Number of positive: 26698, number of negative: 26806\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 53504, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498991 -> initscore=-0.004037\n",
      "[LightGBM] [Info] Start training from score -0.004037\n",
      "[LightGBM] [Info] Number of positive: 26697, number of negative: 26807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 53504, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498972 -> initscore=-0.004112\n",
      "[LightGBM] [Info] Start training from score -0.004112\n",
      "[LightGBM] [Info] Number of positive: 26697, number of negative: 26807\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 53504, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498972 -> initscore=-0.004112\n",
      "[LightGBM] [Info] Start training from score -0.004112\n",
      "LightGBM: Cross-validation metrics calculated\n",
      "[LightGBM] [Info] Number of positive: 33372, number of negative: 33508\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 66880, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498983 -> initscore=-0.004067\n",
      "[LightGBM] [Info] Start training from score -0.004067\n",
      "LightGBM: Test metrics calculated\n",
      "CatBoost: Cross-validation metrics calculated\n",
      "CatBoost: Test metrics calculated\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<Sequential name=sequential, built=False>' (type <class 'keras.src.models.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train and evaluate each model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, model_name \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[1;32m----> 3\u001B[0m     fitted_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     evaluate_on_test_set(fitted_model, model_name, X_test, y_test\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mravel())\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Convert the dictionary of cross-validation metrics to a DataFrame\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 14\u001B[0m, in \u001B[0;36mtrain_and_evaluate_model\u001B[1;34m(model, model_name, X_train, y_train)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Perform cross-validation using StratifiedKFold\u001B[39;00m\n\u001B[0;32m     13\u001B[0m skf \u001B[38;5;241m=\u001B[39m StratifiedKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscoring\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Store the cross-validation metrics\u001B[39;00m\n\u001B[0;32m     17\u001B[0m cv_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(model_name)\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    429\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 430\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[0;32m    448\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    450\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1789\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1786\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1788\u001B[0m \u001B[38;5;66;03m# Sequentially call the tasks and yield the results.\u001B[39;00m\n\u001B[1;32m-> 1789\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   1790\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_dispatched_batches\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m   1791\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_dispatched_tasks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;66;03m# in a different thread depending on the backend and on the value of\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# pre_dispatch and n_jobs.\u001B[39;00m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m---> 63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43m_with_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdelayed_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:432\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    429\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[0;32m    430\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    431\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m--> 432\u001B[0m         \u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    433\u001B[0m         X,\n\u001B[0;32m    434\u001B[0m         y,\n\u001B[0;32m    435\u001B[0m         scorer\u001B[38;5;241m=\u001B[39mscorers,\n\u001B[0;32m    436\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    437\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    438\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m    439\u001B[0m         parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    440\u001B[0m         fit_params\u001B[38;5;241m=\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mfit,\n\u001B[0;32m    441\u001B[0m         score_params\u001B[38;5;241m=\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39mscorer\u001B[38;5;241m.\u001B[39mscore,\n\u001B[0;32m    442\u001B[0m         return_train_score\u001B[38;5;241m=\u001B[39mreturn_train_score,\n\u001B[0;32m    443\u001B[0m         return_times\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    444\u001B[0m         return_estimator\u001B[38;5;241m=\u001B[39mreturn_estimator,\n\u001B[0;32m    445\u001B[0m         error_score\u001B[38;5;241m=\u001B[39merror_score,\n\u001B[0;32m    446\u001B[0m     )\n\u001B[0;32m    447\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m indices\n\u001B[0;32m    448\u001B[0m )\n\u001B[0;32m    450\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\base.py:91\u001B[0m, in \u001B[0;36mclone\u001B[1;34m(estimator, safe)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(estimator, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__sklearn_clone__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misclass(estimator):\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m estimator\u001B[38;5;241m.\u001B[39m__sklearn_clone__()\n\u001B[1;32m---> 91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_clone_parametrized\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - UPV EHU\\23-24\\TFM\\python\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\base.py:113\u001B[0m, in \u001B[0;36m_clone_parametrized\u001B[1;34m(estimator, safe)\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    108\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot clone object. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    109\u001B[0m                 \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should provide an instance of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    110\u001B[0m                 \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscikit-learn estimator instead of a class.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    111\u001B[0m             )\n\u001B[0;32m    112\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 113\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    114\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot clone object \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m): \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    115\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit does not seem to be a scikit-learn \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    116\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mestimator as it does not implement a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    117\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget_params\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m method.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mrepr\u001B[39m(estimator), \u001B[38;5;28mtype\u001B[39m(estimator))\n\u001B[0;32m    118\u001B[0m             )\n\u001B[0;32m    120\u001B[0m klass \u001B[38;5;241m=\u001B[39m estimator\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\n\u001B[0;32m    121\u001B[0m new_object_params \u001B[38;5;241m=\u001B[39m estimator\u001B[38;5;241m.\u001B[39mget_params(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot clone object '<Sequential name=sequential, built=False>' (type <class 'keras.src.models.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "# Train and evaluate each model\n",
    "for model, model_name in models:\n",
    "    fitted_model = train_and_evaluate_model(model, model_name, X_train, y_train.values.ravel())\n",
    "    evaluate_on_test_set(fitted_model, model_name, X_test, y_test.values.ravel())\n",
    "\n",
    "# Convert the dictionary of cross-validation metrics to a DataFrame\n",
    "cv_metrics_df = pd.DataFrame(cv_metrics)\n",
    "test_metrics_df = pd.DataFrame(test_metrics)\n",
    "print(\"\\nCross-validation Metrics:\")\n",
    "print(cv_metrics_df)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(test_metrics_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:37:10.829810Z",
     "start_time": "2024-03-07T09:26:44.597315Z"
    }
   },
   "id": "a085870ac4eda920",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation Metrics:\n",
      "                      Model  Accuracy\n",
      "0       Logistic Regression  0.492344\n",
      "1   Support Vector Machines  0.953768\n",
      "2       K-Nearest Neighbors  0.980173\n",
      "3            Decision Trees  0.974836\n",
      "4             Random Forest  0.975748\n",
      "5         Gradient Boosting  0.943361\n",
      "6           Neural Networks  0.984958\n",
      "7               Naive Bayes  0.800538\n",
      "8                  AdaBoost  0.933463\n",
      "9                   XGBoost  0.981624\n",
      "10                 LightGBM  0.978783\n",
      "11                 CatBoost  0.981160\n",
      "\n",
      "Test Metrics:\n",
      "                      Model  Accuracy\n",
      "0       Logistic Regression  0.486962\n",
      "1   Support Vector Machines  0.957656\n",
      "2       K-Nearest Neighbors  0.983134\n",
      "3            Decision Trees  0.977093\n",
      "4             Random Forest  0.977811\n",
      "5         Gradient Boosting  0.945455\n",
      "6           Neural Networks  0.985766\n",
      "7               Naive Bayes  0.802033\n",
      "8                  AdaBoost  0.937261\n",
      "9                   XGBoost  0.983014\n",
      "10                 LightGBM  0.980502\n",
      "11                 CatBoost  0.984151\n"
     ]
    }
   ],
   "source": [
    "cv_metrics_df = pd.DataFrame(cv_metrics)\n",
    "test_metrics_df = pd.DataFrame(test_metrics)\n",
    "print(\"\\nCross-validation Metrics:\")\n",
    "print(cv_metrics_df)\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(test_metrics_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:42:50.138435Z",
     "start_time": "2024-03-07T09:42:50.130185Z"
    }
   },
   "id": "5d86539e6ac6bcea",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d72d69a4091e9f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
